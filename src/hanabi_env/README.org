* Hanabi Environment

This is an implementation of the Hanabi card game, a cooperative Dec-POMDP, built on the [[https://madrona-engine.github.io/][Madrona game engine]]. This environment is based on [[https://github.com/deepmind/hanabi-learning-environment][Deepmind's Hanabi environment]] and contains some support for [[https://github.com/zoeyuchao/mappo][MAPPO's implementation]].

** Installation

If you have already followed the instructions from the [[file:../../README.md][main README]], Hanabi should already be installed. Otherwise, follow the following directions:

#+begin_src bash
  conda create -n madrona python=3.10
  conda activate madrona
  pip install torch numpy tensorboard

  git clone --recurse-submodules https://github.com/bsarkar321/madrona_rl_envs
  mkdir build && cd build
  cmake ..
  make -j
  cd ..

  pip install -e .
#+end_src

#+begin_quote
On some systems, you may need to specify the cuda toolkit directory for cmake as follows:

#+begin_src bash
  cmake -D CUDAToolkit_ROOT=/usr/local/cuda-12.0 ..
#+end_src
#+end_quote

** Testing

*** Correctness

For testing correctness, ensure that the madrona conda environment is active and you are in the scripts directory.

*For all of the following commands, the last line should be "Error rate: 0.0"*

Verify that gym installation passes tests:
#+begin_src bash
  python hanabi_example.py --num-envs 32 --verbose --validation --asserts --use-baseline
#+end_src

Verify that CPU version passes tests:
#+begin_src bash
  python hanabi_example.py --num-envs 32 --verbose --validation --asserts --use-cpu
#+end_src

Verify that debug GPU version passes tests (note: remove /tmp/bughanabicache if it exists):
#+begin_src bash
  MADRONA_MWGPU_KERNEL_CACHE=/tmp/bughanabicache python hanabi_example.py --num-envs 32 --verbose --validation --asserts --debug-compile
#+end_src

Verify that optimized GPU version passes tests (note: remove /tmp/hanabicache if it exists):
#+begin_src bash
  MADRONA_MWGPU_KERNEL_CACHE=/tmp/hanabicache python hanabi_example.py --num-envs 32 --verbose --validation --asserts
#+end_src

*** Performance

After validating correctness, we can determine the simulation throughput (in terms of steps * worlds / sec). Note that, in general, throughput increases as the number of environments increases.

To test the baseline (gym python implementation), run the following command. Note that Python's multiprocessing library may limit the number of environments you can create; for my system, the optimal throughput was at 32 environments.
#+begin_src bash
  python hanabi_example.py --num-envs 32 --use-baseline
#+end_src

To test the madrona versions for cpu and gpu, run the following commands:
#+begin_src bash
  python hanabi_example.py --num-envs 32 --use-cpu
  MADRONA_MWGPU_KERNEL_CACHE=/tmp/hanabicache python hanabi_example.py --num-envs 32
#+end_src

When testing using 8 cores of an AMD EPYC 7402 Processor, an A40 GPU, and 64 GB RAM, I get the following performance numbers:
| num-envs | baseline | madrona cpu | madrona gpu |
|----------+----------+-------------+-------------|
|       32 | 1500     |       65200 |       55400 |
|      100 | 1500     |       59600 |      155000 |
|     1000 | -        |      310000 |      786000 |
|    10000 | -        |      273000 |     7040000 |
|   100000 | -        |      337000 |    15700000 |

** Training

You can train agents using cleanrl's PPO implementation modified to use these new environments. In particular, you can use hanabi_train.py for decentralized training or hanabi_train_single.py for centralized training.

#+begin_src bash
  MADRONA_MWGPU_KERNEL_CACHE=/tmp/hancache python hanabi_train.py --num-envs 1000 --num-steps 100 --num-updates 1000 --learning-rate 7e-4 --update-epochs 15 --num-minibatches 1 --madrona True --ent-coef 0.015 --anneal-lr False --hanabi-type full
#+end_src

Support for MAPPO will be added soon as well.
